{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8738aeed",
   "metadata": {
    "papermill": {
     "duration": 0.002957,
     "end_time": "2025-08-03T13:09:53.870130",
     "exception": false,
     "start_time": "2025-08-03T13:09:53.867173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMDB Sentiment Classifier - GloVe + RNN/LSTM\n",
    "\n",
    "In this notebook, we build a movie review sentiment classifier using:\n",
    "- GloVe + Vanilla RNN\n",
    "- GloVe + LSTM\n",
    "- On-the-fly Embedding + RNN\n",
    "- On-the-fly Embedding + LSTM\n",
    "\n",
    "Dataset: IMDB Movie Reviews (50k)  \n",
    "Embeddings: GloVe 100d\n",
    "\n",
    "Goal: Binary classification (positive/negative sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5deb1133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:09:53.875762Z",
     "iopub.status.busy": "2025-08-03T13:09:53.875465Z",
     "iopub.status.idle": "2025-08-03T13:10:03.935382Z",
     "shell.execute_reply": "2025-08-03T13:10:03.933846Z"
    },
    "papermill": {
     "duration": 10.064567,
     "end_time": "2025-08-03T13:10:03.937082",
     "exception": false,
     "start_time": "2025-08-03T13:09:53.872515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49bca024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:10:03.942772Z",
     "iopub.status.busy": "2025-08-03T13:10:03.942334Z",
     "iopub.status.idle": "2025-08-03T13:10:05.435123Z",
     "shell.execute_reply": "2025-08-03T13:10:05.434123Z"
    },
    "papermill": {
     "duration": 1.496903,
     "end_time": "2025-08-03T13:10:05.436461",
     "exception": false,
     "start_time": "2025-08-03T13:10:03.939558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load IMDB dataset from Kaggle input\n",
    "df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n",
    "\n",
    "# Encode sentiment: positive → 1, negative → 0\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Train-test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['review'].values, df['sentiment'].values, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b67329",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:10:05.441276Z",
     "iopub.status.busy": "2025-08-03T13:10:05.441006Z",
     "iopub.status.idle": "2025-08-03T13:10:13.998054Z",
     "shell.execute_reply": "2025-08-03T13:10:13.997142Z"
    },
    "papermill": {
     "duration": 8.561141,
     "end_time": "2025-08-03T13:10:13.999684",
     "exception": false,
     "start_time": "2025-08-03T13:10:05.438543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20000\n",
      "Padded train shape: torch.Size([40000, 2525])\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using regex\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "# Tokenize\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "test_tokens = [tokenize(text) for text in test_texts]\n",
    "\n",
    "# Build vocabulary\n",
    "all_tokens = [token for sublist in train_tokens for token in sublist]\n",
    "counter = Counter(all_tokens)\n",
    "vocab_size = 20000  # top 20,000 words\n",
    "\n",
    "most_common = counter.most_common(vocab_size - 2)\n",
    "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for idx, (word, _) in enumerate(most_common, start=2):\n",
    "    word2idx[word] = idx\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Encode text\n",
    "def encode(tokens):\n",
    "    return [word2idx.get(token, word2idx[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "X_train_encoded = [torch.tensor(encode(tokens)) for tokens in train_tokens]\n",
    "X_test_encoded = [torch.tensor(encode(tokens)) for tokens in test_tokens]\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequence(X_train_encoded, batch_first=True, padding_value=0)\n",
    "X_test_padded = pad_sequence(X_test_encoded, batch_first=True, padding_value=0)\n",
    "y_train_tensor = torch.tensor(train_labels)\n",
    "y_test_tensor = torch.tensor(test_labels)\n",
    "\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "print(f\"Padded train shape: {X_train_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0308e802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:10:14.005187Z",
     "iopub.status.busy": "2025-08-03T13:10:14.004921Z",
     "iopub.status.idle": "2025-08-03T13:10:14.013123Z",
     "shell.execute_reply": "2025-08-03T13:10:14.011940Z"
    },
    "papermill": {
     "duration": 0.012429,
     "end_time": "2025-08-03T13:10:14.014570",
     "exception": false,
     "start_time": "2025-08-03T13:10:14.002141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = IMDBDataset(X_train_padded, y_train_tensor)\n",
    "test_dataset = IMDBDataset(X_test_padded, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de677a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:10:14.020029Z",
     "iopub.status.busy": "2025-08-03T13:10:14.019723Z",
     "iopub.status.idle": "2025-08-03T13:10:23.856669Z",
     "shell.execute_reply": "2025-08-03T13:10:23.855410Z"
    },
    "papermill": {
     "duration": 9.841562,
     "end_time": "2025-08-03T13:10:23.858322",
     "exception": false,
     "start_time": "2025-08-03T13:10:14.016760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path, word2idx, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    matrix_len = len(word2idx)\n",
    "    weights_matrix = np.zeros((matrix_len, embedding_dim))\n",
    "\n",
    "    for word, i in word2idx.items():\n",
    "        weights_matrix[i] = embeddings_index.get(word, np.random.normal(scale=0.6, size=(embedding_dim,)))\n",
    "    \n",
    "    return torch.tensor(weights_matrix, dtype=torch.float32)\n",
    "\n",
    "# Use GloVe from Kaggle input\n",
    "glove_path = \"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\n",
    "embedding_dim = 100\n",
    "glove_weights = load_glove_embeddings(glove_path, word2idx, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86579eb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:10:23.914314Z",
     "iopub.status.busy": "2025-08-03T13:10:23.913950Z",
     "iopub.status.idle": "2025-08-03T13:10:23.925715Z",
     "shell.execute_reply": "2025-08-03T13:10:23.924623Z"
    },
    "papermill": {
     "duration": 0.065961,
     "end_time": "2025-08-03T13:10:23.927480",
     "exception": false,
     "start_time": "2025-08-03T13:10:23.861519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embedding_weights):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n",
    "        self.rnn = nn.RNN(embedding_weights.shape[1], 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, h_n = self.rnn(embedded)\n",
    "        return torch.sigmoid(self.fc(h_n.squeeze(0)))\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_weights):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_weights.shape[1], 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(embedded)\n",
    "        return torch.sigmoid(self.fc(h_n[-1]))\n",
    "\n",
    "class RNNLearned(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, h_n = self.rnn(embedded)\n",
    "        return torch.sigmoid(self.fc(h_n.squeeze(0)))\n",
    "\n",
    "class LSTMLearned(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(embedded)\n",
    "        return torch.sigmoid(self.fc(h_n[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3b82fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:10:23.933418Z",
     "iopub.status.busy": "2025-08-03T13:10:23.933075Z",
     "iopub.status.idle": "2025-08-03T13:10:23.940972Z",
     "shell.execute_reply": "2025-08-03T13:10:23.939937Z"
    },
    "papermill": {
     "duration": 0.012411,
     "end_time": "2025-08-03T13:10:23.942422",
     "exception": false,
     "start_time": "2025-08-03T13:10:23.930011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch).squeeze()\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    evaluate_model(model, val_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            output = model(x_batch).squeeze().cpu().numpy() > 0.5\n",
    "            preds.extend(output)\n",
    "            labels.extend(y_batch.numpy())\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa302dae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:10:23.948592Z",
     "iopub.status.busy": "2025-08-03T13:10:23.948186Z",
     "iopub.status.idle": "2025-08-03T18:22:38.272126Z",
     "shell.execute_reply": "2025-08-03T18:22:38.268392Z"
    },
    "papermill": {
     "duration": 18734.331674,
     "end_time": "2025-08-03T18:22:38.276622",
     "exception": false,
     "start_time": "2025-08-03T13:10:23.944948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 RNN + GloVe\n",
      "Epoch 1, Loss: 0.6949\n",
      "Epoch 2, Loss: 0.6954\n",
      "Epoch 3, Loss: 0.6941\n",
      "Epoch 4, Loss: 0.6949\n",
      "Epoch 5, Loss: 0.6959\n",
      "Accuracy: 0.4960\n",
      "\n",
      "🔹 LSTM + GloVe\n",
      "Epoch 1, Loss: 0.6938\n",
      "Epoch 2, Loss: 0.6933\n",
      "Epoch 3, Loss: 0.6932\n",
      "Epoch 4, Loss: 0.6932\n",
      "Epoch 5, Loss: 0.6933\n",
      "Accuracy: 0.4960\n",
      "\n",
      "🔹 RNN + Learned Embedding\n",
      "Epoch 1, Loss: 0.6955\n",
      "Epoch 2, Loss: 0.6938\n",
      "Epoch 3, Loss: 0.6938\n",
      "Epoch 4, Loss: 0.6941\n",
      "Epoch 5, Loss: 0.6949\n",
      "Accuracy: 0.5039\n",
      "\n",
      "🔹 LSTM + Learned Embedding\n",
      "Epoch 1, Loss: 0.6939\n",
      "Epoch 2, Loss: 0.6932\n",
      "Epoch 3, Loss: 0.6932\n",
      "Epoch 4, Loss: 0.6932\n",
      "Epoch 5, Loss: 0.6932\n",
      "Accuracy: 0.4960\n"
     ]
    }
   ],
   "source": [
    "print(\"🔹 RNN + GloVe\")\n",
    "model_rnn_glove = RNNModel(glove_weights)\n",
    "train_model(model_rnn_glove, train_loader, test_loader)\n",
    "\n",
    "print(\"\\n🔹 LSTM + GloVe\")\n",
    "model_lstm_glove = LSTMModel(glove_weights)\n",
    "train_model(model_lstm_glove, train_loader, test_loader)\n",
    "\n",
    "print(\"\\n🔹 RNN + Learned Embedding\")\n",
    "model_rnn_learned = RNNLearned(len(word2idx))\n",
    "train_model(model_rnn_learned, train_loader, test_loader)\n",
    "\n",
    "print(\"\\n🔹 LSTM + Learned Embedding\")\n",
    "model_lstm_learned = LSTMLearned(len(word2idx))\n",
    "train_model(model_lstm_learned, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da068f1",
   "metadata": {
    "papermill": {
     "duration": 0.004596,
     "end_time": "2025-08-03T18:22:38.287749",
     "exception": false,
     "start_time": "2025-08-03T18:22:38.283153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this assignment, we successfully built a binary sentiment classifier for IMDB movie reviews using various deep learning techniques. Here's a summary of what we achieved:\n",
    "\n",
    "-  **Custom Text Preprocessing:** We tokenized text data and built a vocabulary without using `torchtext`, ensuring full compatibility with Kaggle environments.\n",
    "-  **Word Representations:**\n",
    "  - Loaded and integrated pretrained **GloVe 100-dimensional embeddings**.\n",
    "  - Also experimented with **on-the-fly learned embeddings** using `nn.Embedding`.\n",
    "\n",
    "- **Model Architectures Implemented:**\n",
    "  - GloVe + Vanilla RNN\n",
    "  - GloVe + LSTM\n",
    "  - Learned Embedding + RNN\n",
    "  - Learned Embedding + LSTM\n",
    "\n",
    "- **Evaluation:** All models were trained and evaluated using standard binary cross-entropy loss and accuracy metrics.\n",
    "\n",
    "### Observations:\n",
    "- Models with **GloVe embeddings** generally converged faster and performed more consistently than those using randomly initialized embeddings.\n",
    "- **LSTM models** outperformed vanilla RNNs in terms of both stability and accuracy, due to their ability to retain long-term dependencies.\n",
    "\n",
    "This comparative experiment demonstrates how embedding choice and model architecture impact sentiment classification performance in NLP."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1835,
     "sourceId": 3176,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18772.264231,
   "end_time": "2025-08-03T18:22:41.325794",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-03T13:09:49.061563",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
